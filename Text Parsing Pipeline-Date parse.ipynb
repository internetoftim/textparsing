{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import en_core_web_sm\n",
    "import keras\n",
    "import tensorflow\n",
    "from pathlib import Path\n",
    "from pathlib import PureWindowsPath\n",
    "import scrapy\n",
    "from dateparser.search import search_dates\n",
    "import arrow\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_html(input_html):\n",
    "    with open(input_html) as inf:\n",
    "        txt = inf.read()\n",
    "    soup_out = soup(txt, \"html.parser\")\n",
    "    print(input_html)\n",
    "    return soup_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_firm(input_soup):\n",
    "    temp = input_soup.find(id='article_participants')\n",
    "    if temp.findAll('a')==[]:\n",
    "        temp = input_soup.find(id='article_content')\n",
    "    for element in temp.findAll('a'):\n",
    "        if element.get('title') != None :\n",
    "            return(str(element.get('title')))   \n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ticker(input_soup):\n",
    "    temp = input_soup.find(id='article_participants')\n",
    "    if temp.findAll('a')==[]:\n",
    "        temp = input_soup.find(id='article_content')\n",
    "    for element in temp.findAll('a'):\n",
    "        if element.get('title') != None :\n",
    "            return(str(element.contents[0].strip()))   \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dateparser\n",
    "def extract_time(input_soup):\n",
    "    temp = input_soup.find(id='article_participants')\n",
    "    for element in temp.find_all('p'):\n",
    "        temp_string=element.text.strip()\n",
    "        try:\n",
    "#             December 2, 2014 11:15 AM ET\n",
    "            temp_string = re.match(string=temp_string,pattern='([a-zA-Z]+\\s+[0-9]+,\\s+[0-9]{4})[^ 0-9]?(\\s+[0-9]{1,2}:[0-9]{2}\\s+[APap]\\.?[mM]\\.?)')\n",
    "            string1 = temp_string.group(1)\n",
    "            string2 = temp_string.group(2)\n",
    "            #         name_search.group(1).strip()\n",
    "            string3 = string1 + string2\n",
    "            string3 = re.sub(string=string3,pattern='\\.',repl='')\n",
    "            string3 = re.sub(string=string3,pattern='\\s+',repl=' ')\n",
    "#             print(string1,'##',string2,'##',string3)\n",
    "            date_out = arrow.get(string3,'MMMM D, YYYY H:mm A')\n",
    "        except:\n",
    "            date_out=None        \n",
    "            \n",
    "        if date_out is not None:\n",
    "            return date_out.isoformat()\n",
    "    temp = input_soup.find(id='article_content')\n",
    "    \n",
    "    for element in temp.find_all('a'):\n",
    "        temp_string=element.text.strip()\n",
    "        temp_string = re.sub(string=temp_string,pattern='\\s+',repl=' ')\n",
    "        try:\n",
    "            while  (element):\n",
    "                try:\n",
    "                    date_out = arrow.get(temp_string,'MMMM D, YYYY H:mm A')\n",
    "                except:\n",
    "                    date_out=None       \n",
    "                if date_out is not None:\n",
    "    #                 return temp_string\n",
    "                    return date_out.isoformat()\n",
    "\n",
    "\n",
    "                element = element.next_sibling\n",
    "                temp_string = element\n",
    "                temp_string = re.sub(string=temp_string,pattern='\\s+',repl=' ')  \n",
    "    #             print(temp_string.find('div'))\n",
    "    #             print(temp_string.find('strong'))\n",
    "        except:\n",
    "            pass\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_quarter(input_soup):\n",
    "    temp = input_soup.find(id='article_participants')\n",
    "    try:\n",
    "        for element in temp.findAll('p'):\n",
    "            if (pd.notnull(element.text) and element.text!=''):\n",
    "                if( re.search('call',element.contents[0],flags=re.IGNORECASE)):\n",
    "                    call_title = element.contents[0].split()\n",
    "                    return str(call_title[0])\n",
    "        return None\n",
    "    except:\n",
    "        temp = input_soup.find(id='article_participants')\n",
    "\n",
    "        return None\n",
    "    \n",
    "def extract_year(input_soup):\n",
    "    temp = input_soup.find(id='article_participants')\n",
    "    try:\n",
    "        for element in temp.findAll('p'):\n",
    "            if (pd.notnull(element.text) and element.text!=''):\n",
    "                if( re.search('call',element.contents[0],flags=re.IGNORECASE)):\n",
    "                    call_title = element.contents[0].split()\n",
    "                    return str(call_title[1])\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_analyst(input_soup):\n",
    "    temp = input_soup.find(id='article_participants')\n",
    "    for element in temp.findAll('strong'):\n",
    "        if(re.search('analyst',element.contents[0],flags=re.IGNORECASE)):\n",
    "            return(True)\n",
    "    return(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_analysts(input_soup):\n",
    "    temp = input_soup.find(id='article_participants')\n",
    "    test_string = []\n",
    "    print_flag = False\n",
    "    for element in temp.find_all('p'):\n",
    "        if(re.search('analyst',element.text,flags=re.IGNORECASE)):\n",
    "            #print(element.text.strip(),'###')\n",
    "            element = element.find_next()\n",
    "            while  (1):\n",
    "                element = element.find_next()\n",
    "            #print(element.contents[0].strip())\n",
    "            #print(element.text.strip())\n",
    "                if element.find('strong') or \\\n",
    "                element.find('div'):\n",
    "            #print('FOUND')\n",
    "                    break\n",
    "                if (pd.notnull(element.text) and element.text!=''):\n",
    "                    test_string.append(element.contents[0].strip())    \n",
    "            break\n",
    "    return test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_executives(input_soup):\n",
    "    test_string = []\n",
    "    temp = input_soup.find(id='article_participants')\n",
    "    # print(soup_out)\n",
    "    print_flag = False\n",
    "    for element in temp.find_all('p'):\n",
    "        if(re.search('executive',element.text,flags=re.IGNORECASE)):\n",
    "            #print(element.text.strip(),'###')\n",
    "            element = element.find_next()\n",
    "            while  (1):\n",
    "                element = element.find_next()\n",
    "                # print(element.contents[0])\n",
    "                # print(element.text.strip())\n",
    "                if element.find('strong') or \\\n",
    "                element.find('div'):            \n",
    "                #print('FOUND')\n",
    "                    break\n",
    "                test_string.append(element.text.strip())\n",
    "        \n",
    "            break\n",
    "    return test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_analyst_present(analyst_list):\n",
    "    if analyst_list: \n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_name (input_string,delim='-'):\n",
    "    if not pd.isnull(input_string):\n",
    "        name_search = re.search('(^[^-]*)-', input_string, re.IGNORECASE)\n",
    "        try:\n",
    "            return name_search.group(1).strip()\n",
    "        except:\n",
    "            return None        \n",
    "    else:\n",
    "        return None\n",
    "def extract_title (input_string,delim='-'):\n",
    "    if not pd.isnull(input_string):\n",
    "        title_search = re.search('^[^-]*-(.*)', input_string, re.IGNORECASE)\n",
    "        try:\n",
    "            return title_search.group(1).strip()\n",
    "        except:\n",
    "            return None           \n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_content(input_soup,current_name):\n",
    "#     current_name = 'Panna Sharma'\n",
    "    if not (pd.isnull(current_name) ):\n",
    "        temp = input_soup.find(id='article_content')\n",
    "        test_string = []\n",
    "        # print(temp)\n",
    "        for element in temp.find_all('strong'):\n",
    "            if (re.search(current_name.strip().lower(),element.text,flags=re.IGNORECASE)):\n",
    "                while  (1):\n",
    "                    element = element.find_next()\n",
    "        #           print(element.text.strip())\n",
    "                    if element.find('strong') or \\\n",
    "                    element.find('div'):\n",
    "                        break\n",
    "                    test_string.append(element.text.strip())    \n",
    "        return test_string\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_qanda(input_soup,current_name):\n",
    "#     current_name = 'Panna Sharma'\n",
    "    if not (pd.isnull(current_name) ):\n",
    "        temp = input_soup.find(id='article_qanda')\n",
    "        test_string = []\n",
    "        # print(temp)\n",
    "        if temp.find_all('span'):\n",
    "            for element in temp.find_all('span'):\n",
    "                if (re.search(current_name.strip().lower(),element.text,flags=re.IGNORECASE)):\n",
    "                    while  (1):\n",
    "                        element = element.find_next()\n",
    "            #           print(element.text.strip())\n",
    "                        if element.find('strong') or \\\n",
    "                        element.find('div') or \\\n",
    "                        element.find('span'):\n",
    "                            break\n",
    "                        test_string.append(element.text.strip())    \n",
    "        else:                \n",
    "            for element in temp.find_all('strong'):\n",
    "                if (re.search(current_name.strip().lower(),element.text,flags=re.IGNORECASE)):\n",
    "                    while  (1):\n",
    "                        element = element.find_next()\n",
    "            #           print(element.text.strip())\n",
    "                        if element.find('strong') or \\\n",
    "                        element.find('div') or \\\n",
    "                        element.find('span'):\n",
    "                            break\n",
    "                        test_string.append(element.text.strip())      \n",
    "            \n",
    "        return test_string\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def basic_wc(input_text):\n",
    "    if not (input_text==[]):\n",
    "#         decoded_txt=input_text.decode('utf-8')\n",
    "        sent =  \" \".join(str(x) for x in input_text)\n",
    "        sent = re.sub(pattern='[^a-zA-Z0-9 -]',repl='',string=sent)            \n",
    "        doc = nlp(sent)\n",
    "        return doc.__len__()\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_chunks(input_text):\n",
    "    sent =  \" \".join(str(x) for x in input_text)\n",
    "    #     doc = nlp(input_text)\n",
    "    doc = nlp(sent)\n",
    "    spans = list(doc.ents)  #+ list(doc.noun_chunks)\n",
    "    out_list = []\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "        out_list.append(span)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_name_pair(name, input_text):\n",
    "    out_list = []\n",
    "    if not (input_text==[]):\n",
    "#         s_out = extract_chunks(input_text)  \n",
    "        sent =  \" \".join(str(x) for x in input_text)\n",
    "        #     doc = nlp(input_text)\n",
    "        doc = nlp(sent)        \n",
    "#         doc = nlp(input_text)\n",
    "        spans = list(doc.ents)  #+ list(doc.noun_chunks)\n",
    "\n",
    "        for item in spans:\n",
    "            try:\n",
    "                item = re.sub(pattern='[^a-zA-Z0-9 -]',repl='',string=item.string)            \n",
    "                if (re.search(item,name,flags=re.IGNORECASE)):\n",
    "                    out_list.append(item)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    return out_list\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38490\n"
     ]
    }
   ],
   "source": [
    "collection = \"html\"\n",
    "\n",
    "# n_test=50\n",
    "file_list = os.listdir(collection)\n",
    "# for i in range(len(file_list)):\n",
    "# for i in range(n_test):\n",
    "#     print(file_list[i])\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE\tANALYST_Flag\tFIRM\tQuarter\tYear\n",
      "91979\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "fieldnames = ['index','fname', 'status']\n",
    "# output_folder = PureWindowsPath('C:/Users/T.Santos18/Documents/workspace/textparsing/exec')\n",
    "output_folder = PureWindowsPath('C:/Users/T.Santos18/Documents/workspace/textparsing/dates')\n",
    "input_folder = PureWindowsPath('C:/Users/T.Santos18/Documents/workspace/textparsing/html')\n",
    "file_list = os.listdir(input_folder)\n",
    "out_list =[]\n",
    "# for i in range(len(file_list)):\n",
    "print('FILE\\tANALYST_Flag\\tFIRM\\tQuarter\\tYear') \n",
    "print(len(file_list))\n",
    "for i in range(len(file_list)):\n",
    "    with open(input_folder / file_list[i]) as inf:\n",
    "        if(re.search('html',file_list[i],flags=re.IGNORECASE)):\n",
    "            out_list.append(file_list[i])\n",
    "#             print(file_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# out_analyst.drop(['text','analyst_present','analysts','executive'],axis=1)\n",
    "out_executives.drop(['text','executive','execs'],axis=1)\\\n",
    ".to_csv('analyst_out_foringestion.csv',sep=',', encoding = 'utf-8')\n",
    "# out_executives.select({[}'file','quarter_yer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive Z is Data\n",
      " Volume Serial Number is 647E-DA71\n",
      "\n",
      " Directory of Z:\\cse315\\textparsing\n",
      "\n",
      "10/03/2018  03:06 PM    <DIR>          .\n",
      "10/03/2018  03:06 PM    <DIR>          ..\n",
      "10/03/2018  02:58 PM    <DIR>          .git\n",
      "09/21/2018  06:40 PM    <DIR>          .ipynb_checkpoints\n",
      "09/19/2018  01:35 PM    <DIR>          html\n",
      "09/19/2018  01:35 PM                 0 log_extract.csv\n",
      "09/19/2018  01:02 PM                 0 log_ner.csv\n",
      "09/22/2018  05:02 AM            10,502 Named Entity Recognition Pipeline.ipynb\n",
      "09/22/2018  01:12 AM            10,493 Named Entity Recognition Pipeline-Copy1.ipynb\n",
      "09/22/2018  12:42 AM            10,502 Named Entity Recognition Pipeline-Copy2.ipynb\n",
      "09/22/2018  01:28 AM            10,504 Named Entity Recognition Pipeline-Copy3.ipynb\n",
      "09/19/2018  01:02 PM             9,337 output_0912.xlsx\n",
      "09/21/2018  01:22 PM           267,033 Parse Conversation HTML.ipynb\n",
      "09/19/2018  01:02 PM                58 README.md\n",
      "09/19/2018  01:03 PM    <DIR>          sample_out\n",
      "09/19/2018  01:02 PM         3,211,605 Text Parsing Pipeline Analyst.ipynb\n",
      "10/03/2018  02:56 PM            30,360 Text Parsing Pipeline.ipynb\n",
      "10/03/2018  03:06 PM           605,094 Text Parsing Pipeline-Copy1.ipynb\n",
      "09/20/2018  04:53 PM         1,561,227 Text Parsing Pipeline-Copy2.ipynb\n",
      "09/19/2018  05:37 PM           168,517 Text Parsing Pipeline-Copy3.ipynb\n",
      "              14 File(s)      5,895,232 bytes\n",
      "               6 Dir(s)  13,766,246,400 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "fieldnames = ['index','fname', 'status']\n",
    "# output_folder = PureWindowsPath('C:/Users/T.Santos18/Documents/workspace/textparsing/exec')\n",
    "output_folder = PureWindowsPath('C:/Users/T.Santos18/Documents/workspace/textparsing/dates')\n",
    "input_folder = PureWindowsPath('C:/Users/T.Santos18/Documents/workspace/textparsing/html')\n",
    "\n",
    "with open(output_folder / 'log_extract.csv', 'w') as csvfile:\n",
    "\n",
    "    logwriter = csv.writer(csvfile,delimiter='\\t',quotechar='\"',quoting=csv.QUOTE_MINIMAL)\n",
    "    for ii in range(1,20000):\n",
    "        try:\n",
    "            total_rows = len(file_list)\n",
    "            row_start = ii*1\n",
    "            row_end = (ii*1)+1\n",
    "            filename = 'exec_out_foringestion_'+str(row_start)+'.csv'\n",
    "\n",
    "            out_list =[]\n",
    "\n",
    "            for i in range(row_start,row_end):\n",
    "                with open(input_folder / file_list[i]) as inf:\n",
    "#                 with open(collection + '/' + file_list[i]) as inf:\n",
    "                    if(re.search('html',file_list[i],flags=re.IGNORECASE)):\n",
    "                        out_list.append(file_list[i])\n",
    "\n",
    "            out_pd = pd.DataFrame(out_list)\n",
    "            out_pd.columns=['file']\n",
    "            out_pd['text'] = out_pd.apply(lambda x: extract_html(input_folder / x['file']), axis=1)\n",
    "            out_pd['quarter'] = out_pd.apply(lambda x: extract_quarter(x['text']), axis=1)\n",
    "            out_pd['year'] = out_pd.apply(lambda x: extract_year(x['text']), axis=1)\n",
    "            out_pd['datestamp'] = out_pd.apply(lambda x: extract_time(x['text']), axis=1)\n",
    "            out_pd['executive_firm'] = out_pd.apply(lambda x: extract_firm(x['text']), axis=1)\n",
    "            out_pd['ticker'] = out_pd.apply(lambda x: extract_ticker(x['text']), axis=1)\n",
    "            \n",
    "            out_executives = out_pd                      \n",
    "            out_executives.drop(['text'],axis=1).to_csv(output_folder / filename,sep=',', encoding = 'utf-8',header=False)\n",
    "            logwriter.writerow([ii,out_executives.file.head(1).item(),'success'])\n",
    "        except:\n",
    "            print('exception: ', ii, ' ###')\n",
    "            logwriter.writerow([ii,out_executives.file.head(1).item(),'fail'])\n",
    "            \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "dateformat = '%B %d, %Y %I:%M %p %Z'\n",
    "temp_date = 'Sage Group plc ( OTC:SGGEF ) Q4 2014 Earnings Conference Call December 3, 2014 3:45 AM ET'\n",
    "\n",
    "date_out = dateparser.parse(date_string=temp_date,settings={'DATE_ORDER': 'MDY', 'PREFER_LANGUAGE_DATE_ORDER': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_out = arrow.get(temp_date,'MMMM D, YYYY H:mm A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-12-03T03:45:00+00:00'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_out.isoformat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
